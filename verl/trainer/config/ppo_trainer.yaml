# Functionality from HEAD branch - extensive PPO trainer configurations
actor_rollout_ref:
  # for huge model, layered summon can save memory (prevent OOM) but make it slower
  layered_summon: False

  # TP size for rollout. Only effective for vLLM.
  tensor_model_parallel_size: 2

  # max number of tokens in a batch
  max_num_batched_tokens: 8192

  # max length for rollout
  max_model_len: null

  # max length of sequences
  max_num_seqs: 1024

  # [Will be deprecated, use log_prob_micro_batch_size_per_gpu] The batch size for one forward pass in the computation of log_prob. Global batch size.
  log_prob_micro_batch_size: null

  # The batch size for one forward pass in the computation of log_prob. Local batch size per GPU.
  log_prob_micro_batch_size_per_gpu: null

  # enable dynamic batch size (sequence packing) for log_prob computation
  log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}

  # max token length for log_prob computation
  log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}

  # disable logging statistics
  disable_log_stats: True

  # may get higher throughput when set to True. When activated, Please increase max_num_batched_tokens or decrease max_model_len.
  enable_chunked_prefill: True

  # for hf rollout
  # Whether to sample during training rollout. False uses greedy sampling.
  do_sample: True

  # number of responses (i.e. num sample times). > 1 for grpo
  n: 1

  # Whether to wake up inference engine in multi-stage. (Wake up model weights first, then resume kv cache)
  multi_stage_wake_up: false

  # Extra inference engine arguments (vllm, sglang).
  engine_kwargs:

    # for vllm
    vllm:

      # Swap space (in GB) used by inference engine. null uses default (e.g., 4 GB).
      swap_space: null

      # Whether to disable the preprocessor cache for multimodel models.
      disable_mm_preprocessor_cache: False

    # for sglang
    sglang:

      # The attention backend for sglang engine. Options: flashinfer, triton, flashmla, null for default.
      attention_backend: null

  # Sampling parameters used during validation.
  val_kwargs:

    # sampling parameters for validation
    # Top-k sampling parameter. -1 for vLLM rollout, 0 for HF rollout.
    top_k: -1

    # Top-p sampling parameter. Default 1.0.
    top_p: 1.0

    # Sampling temperature for rollout.
    temperature: 0

    # whether to repeat n times for validation
    n: 1

    # Whether to sample during training rollout. False uses greedy sampling.
    do_sample: False

  # Multi-turn interaction config for tools or chat.
  multi_turn:

    # set to True for multi-turn tool interaction tasks; should set rollout.name to sglang as well
    enable: False

    # null for no limit (default max_length // 3)
    max_assistant_turns: null

    # null for no tool
    tool_config_path: null

    # null for no limit (default max_length // 3)
    max_user_turns: null

    # max parallel call for tools in single turn
    max_parallel_calls: 1

    # max length of tool response
    max_tool_response_length: 256

    # truncate side of tool response: left, middle, right
    tool_response_truncate_side: middle

    # null for no interaction
    interaction_config_path: null

    # null for default callback
    completion_callback: null

    # - When set to True, the model's default chat template is used for multi-turn rollout, which typically matches production behavior.
    # - When set to False, the token ids recorded for training are used instead; unlike the default chat template, these always include the model's full output,
    #   which may contain additional content such as reasoning content. This maintains the consistency between training and rollout, but it will lead to longer prompts.
    use_inference_chat_template: False

    # Tokenization is performed turn by turn and the resulting token ids are concatenated to form the full conversation.
    # To ensure this matches the result of tokenizing the entire conversation at once, a sanity check is run at the end of each multi-turn rollout to compare the two sets of token ids.
    # Some models are known to produce different tokenization results when tokenizing turn by turn vs. all at once. aThis behavior has already been validated for them.
    # To reduce excessive warnings, you can turn off the sanity check for these models if you are using their default chat template:
    # Qwen/QwQ-32B, Qwen/Qwen3-xxB
    # - disable: disable tokenization sanity check
    # - strict: enable strict tokenization sanity check (default)
    # - ignore_strippable: ignore strippable tokens when checking tokenization sanity
    tokenization_sanity_check_mode: strict

    # Format of the multi-turn interaction. Options: hermes, llama3_json, ...
    format: hermes

  # support logging rollout prob for debugging purpose
  calculate_log_probs: False

  # profiler configs
  profiler:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs in the entrypoint
    _target_: verl.utils.profiler.ProfilerConfig

    # True for each task has its own database, False for all tasks in one training step share one database.
    discrete: False

    # Whether to profile all ranks.
    all_ranks: False

    # The ranks that will be profiled. [] or [0,1,...]
    ranks: []

  # [Experimental] agent loop based rollout configs
  agent:

    # Number of agent loop workers
    num_workers: 8

    # custom async server configs
    custom_async_server:

      # Path to the custom async server implementation
      path: null

      # Class name of the custom async server class (e.g. AsyncvLLMServer)
      name: null

# configs for the critic
critic:

  # Number of rollouts per update (mirrors actor rollout_n)
  rollout_n: ${actor_rollout_ref.rollout.n}

  # fsdp or fsdp2 strategy used for critic model training
  strategy: ${actor_rollout_ref.actor.strategy}

  # optimizer configs
  optim:

    # Learning rate
    lr: 1e-5

    # Warmup steps ratio; total steps will be injected at runtime
    lr_warmup_steps_ratio: 0.

    # Minimum LR ratio for cosine schedule
    min_lr_ratio: null

    # LR warmup style: "constant" or "cosine"
    warmup_style: constant

    # Total training steps (must be overridden at runtime)
    total_training_steps: -1

    # Weight decay
    weight_decay: 0.01

  # model config for the critic
  model:

    # Path to pretrained model weights
    path: ~/models/deepseek-llm-7b-chat

    # Whether to use shared memory for loading the model
    use_shm: False

    # Tokenizer path (defaults to actor's model path)
    tokenizer_path: ${actor_rollout_ref.model.path}

    # Hugging Face config override
    override_config: { }

    # External model implementation (optional)
    external_lib: ${actor_rollout_ref.model.external_lib}

    # Enable gradient checkpointing to save memory
    enable_gradient_checkpointing: True

    # Offload activations to CPU to reduce GPU memory usage
    enable_activation_offload: False

    # Use remove padding optimization (saves compute)
    use_remove_padding: False

    # Whether to trust remote code from Hugging Face models
    trust_remote_code: ${actor_rollout_ref.model.trust_remote_code}

    # FSDP-specific config
    fsdp_config:

      # Whether to offload model parameters to CPU
      param_offload: False

      # Whether to offload optimizer state to CPU
      optimizer_offload: False

      # Only for FSDP2: offload param/grad/optimizer during train
      offload_policy: False

      # Only for FSDP2: Reshard after forward pass to reduce memory footprint
      reshard_after_forward: True

      # Policy for wrapping layers with FSDP
      wrap_policy:

        # Minimum number of parameters to trigger wrapping
        min_num_params: 0

      # Number of GPUs in each FSDP shard group; -1 means auto
      fsdp_size: -1

      # Only for FSDP1: FSDP1 configuration, prefetch the next forward-pass all-gather
      # before the current forward computation.
      forward_prefetch: False

    # Set to positive value to enable LoRA (e.g., 32)
    lora_rank: 0

    # LoRA scaling factor
    lora_alpha: 16

    # LoRA target modules: "all-linear" or list of linear projection layers
    target_modules: all-linear

  # PPO mini-batch size per update
  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}

  # [Deprecated] Global micro batch size
  ppo_micro_batch_size: null

  # Local per-GPU micro batch size
  ppo_micro_batch_size_per_gpu: null

  # Forward-only batch size (global)
  forward_micro_batch_size: ${critic.ppo_micro_batch_size}

  # Forward-only batch size (per GPU)
  forward_micro_batch_size_per_gpu: ${critic.ppo_micro_batch_size_per_gpu}

  # Whether to automatically adjust batch size at runtime
  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}

  # Max tokens per GPU in one PPO batch (doubled for critic)
  ppo_max_token_len_per_gpu: 32768

  # Max token length per GPU in forward pass
  forward_max_token_len_per_gpu: ${critic.ppo_max_token_len_per_gpu}

  # Sequence parallelism size for Ulysses-style model parallelism
  ulysses_sequence_parallel_size: 1

  # Number of PPO epochs per batch
  ppo_epochs: ${actor_rollout_ref.actor.ppo_epochs}

  # Shuffle training data across PPO epochs
  shuffle: ${actor_rollout_ref.actor.shuffle}

  # Gradient clipping for critic updates
  grad_clip: 1.0

  # PPO value function clipping range
  cliprange_value: 0.5

  # Loss aggregation mode: "token-mean", "seq-mean-token-sum", or "seq-mean-token-mean"
  loss_agg_mode: ${actor_rollout_ref.actor.loss_agg_mode}

  # checkpoint configs
  checkpoint:

    # What to include in saved checkpoints
    # with 'hf_model' you can save whole model as hf format, now only use sharded model checkpoint to save space
    save_contents: ['model', 'optimizer', 'extra']

    # What to include when loading checkpoints
    load_contents: ${critic.checkpoint.save_contents}

  # profiler configs
  # the corresponding dataclass is verl.utils.profiler.ProfilerConfig.
  profiler:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs in the entrypoint
    _target_: verl.utils.profiler.ProfilerConfig

    # True for each task has its own database, False for all tasks in one training step share one database.
    discrete: False

    # Whether to profile all ranks.
    all_ranks: False

    # The ranks that will be profiled. [] or [0,1,...]
    ranks: []

# configs for the reward model
reward_model:

  # Whether to enable reward model. If False, we compute the reward only with the user-defined reward functions.
  # In GSM8K and Math examples, we disable reward model.
  # For RLHF alignment example using full_hh_rlhf, we utilize reward model to assess the responses.
  # If False, the following parameters are not effective
  enable: False

  # FSDP strategy: "fsdp" or "fsdp2"
  strategy: ${actor_rollout_ref.actor.strategy}

  # model config for reward scoring
  model:

    # Input tokenizer. If the reward model’s chat template is inconsistent with the policy,
    # we need to first decode to plaintext, then apply the rm’s chat_template.
    # Then score with RM. If chat_templates are consistent, it can be set to null.
    input_tokenizer: ${actor_rollout_ref.model.path}

    # RM’s HDFS path or local path. Note that RM only supports AutoModelForSequenceClassification.
    # Other model types need to define their own RewardModelWorker and pass it from the code.
    path: ~/models/FsfairX-LLaMA3-RM-v0.1

    # Whether to use shared memory for loading the model
    use_shm: False

    # External model implementation (optional)
    external_lib: ${actor_rollout_ref.model.external_lib}

    # Use remove padding optimization (saves compute)
    use_remove_padding: False

    # Whether to use fused reward kernels for speedup
    use_fused_kernels: ${actor_rollout_ref.model.use_fused_kernels}

    # Whether to enable loading a remote code model, default to False
    trust_remote_code: False

    # FSDP-specific config
    fsdp_config:

      # Policy for wrapping layers with FSDP
      wrap_policy:

        # Minimum number of parameters to trigger wrapping
        min_num_params: 0

      # Whether to offload model parameters to CPU
      param_offload: False

      # Only for FSDP2: Reshard after forward pass to reduce memory footprint
      reshard_after_forward: True

      # Number of GPUs in each FSDP shard group; -1 means auto
      fsdp_size: -1

      # Only for FSDP1: FSDP1 configuration, prefetch the next forward-pass all-gather
      # before the current forward computation.
      forward_prefetch: False

  # [Deprecated] Global micro batch size
  micro_batch_size: null

  # Local per-GPU micro batch size
  micro_batch_size_per_gpu: null

  # Maximum sequence length to process for scoring
  max_length: null

  # enable dist reward worker
  enable_reward_workers: False

  # Sequence parallelism size for Ulysses-style model parallelism
  ulysses_sequence_parallel_size: 1

  # Whether to dynamically adjust batch size at runtime
  use_dynamic_bsz: ${critic.use_dynamic_bsz}

  # Maximum number of tokens per GPU in one forward pass
  forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}

  # Reward Manager. This defines the mechanism of computing rule-based reward and handling different reward sources.
  # Default is naive. If all verification functions are multiprocessing-safe,
  # the reward manager can be set to prime for parallel verification.
  reward_manager: naive

  # Whether to launch custom reward function asynchronously during log_prob
  launch_reward_fn_async: False

  # Cloud/local sandbox fusion configuration for custom reward logic
  sandbox_fusion:

    # Cloud/local function URL for sandbox execution
    url: null

    # Max concurrent requests allowed to sandbox
    max_concurrent: 64

    # Max memory limit for each sandbox process in MB
    memory_limit_mb: 1024

  # profiler configs
  profiler:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs in the entrypoint
    _target_: verl.utils.profiler.ProfilerConfig

    # True for each task has its own database, False for all tasks in one training step share one database.
    discrete: False

    # Whether to profile all ranks.
    all_ranks: False

    # The ranks that will be profiled. [] or [0,1,...]
    ranks: []

# Functionality from main branch - custom reward function definition
# custom reward function definition
custom_reward_function:
  # This example reward function config shows how to use custom reward function.
  # A custom reward function can be defined in verl.utils.reward_score and registered with register_reward_function.
  # The reward function should follow the signature:
  # def reward_function(data: DataProto, **kwargs) -> torch.Tensor:
  #     ...
  #     return rewards  # shape: [batch_size]

  # Example for GSM8K reward function
  # _target_: verl.utils.reward_score.gsm8k.compute_math_reward
  # Example for Math reward function
  # _target_: verl.utils.reward_score.math_batch.compute_math_reward_batch
  _target_: null

  # Additional kwargs for the reward function
  kwargs: { }
